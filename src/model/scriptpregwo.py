# -*- coding: utf-8 -*-
"""ScriptPreGWO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AAEGEBgXZ6oQxX1C2ceDYhSUAmahL0d_
"""

import pandas as pd
import numpy as np

# Cargar el archivo server_usage
server_usage = pd.read_csv('/content/server_usage_filtered.csv')

# Cargar el archivo batch_task
batch_task = pd.read_csv('/content/batch_task_filtered.csv')

# Ver las primeras filas de cada archivo para verificar su contenido
print(server_usage)
print(batch_task)

# Renombrar columnas en server_usage.csv
server_usage.columns = ['timestamp', 'machine_id', 'cpu_usage_percent', 'memory_usage_percent', 'disk_usage_percent',
                        'cpu_load_1min', 'cpu_load_5min', 'cpu_load_15min']

# Renombrar columnas en batch_task.csv
batch_task.columns = ['task_create_time', 'task_end_time', 'job_id', 'task_id', 'num_instances', 'status',
                      'cpus_requested', 'memory_requested']

# Verificar los cambios
print(server_usage.head())
print(batch_task.head())

# Imputar valores nulos con la media para cada columna en server_usage
for column in server_usage.columns:
    if server_usage[column].isnull().any():
        mean_value = server_usage[column].mean()
        server_usage[column].fillna(mean_value, inplace=True)

# Imputar valores nulos con la media para cada columna en batch_task
for column in batch_task.columns:
    if batch_task[column].isnull().any():
        mean_value = batch_task[column].mean()
        batch_task[column].fillna(mean_value, inplace=True)

# Verificar si hay valores nulos después de la imputación
print(server_usage.isnull().sum())
print(batch_task.isnull().sum())

from sklearn.preprocessing import MinMaxScaler

# Crear un objeto MinMaxScaler
scaler = MinMaxScaler()

# Escalar las columnas numéricas de server_usage
numeric_cols_server = ['cpu_usage_percent', 'memory_usage_percent', 'disk_usage_percent',
                        'cpu_load_1min', 'cpu_load_5min', 'cpu_load_15min']
server_usage[numeric_cols_server] = scaler.fit_transform(server_usage[numeric_cols_server])

# Escalar las columnas numéricas de batch_task
numeric_cols_batch = ['num_instances', 'cpus_requested', 'memory_requested']
batch_task[numeric_cols_batch] = scaler.fit_transform(batch_task[numeric_cols_batch])

# Verificar los cambios
print(server_usage.head())
print(batch_task.head())

# Guardar los DataFrames preprocesados en nuevos archivos CSV
server_usage.to_csv('/content/preprocessed_server_usage.csv', index=False)

batch_task.to_csv('/content/preprocessed_batch_task.csv', index=False)

# Parámetros y datos
max_tasks = 1500
max_servers = 200
n_iterations = 30
a = 2 * np.exp(-0.05)
a_decay = (2 - 0.1) / n_iterations # Calcula la tasa de decaimiento de a

batch_task = pd.read_csv('/content/preprocessed_batch_task.csv', nrows=max_tasks)
print(batch_task.head())  # Verifica que los datos sean cargados correctamente

server_usage = pd.read_csv('/content/preprocessed_server_usage.csv', nrows=max_servers)
print(server_usage.head())  # Verifica que los datos sean cargados correctamente

server_usage
batch_task